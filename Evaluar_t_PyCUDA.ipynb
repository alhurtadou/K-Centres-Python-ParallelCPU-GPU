{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "  !pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_HeEFMAkUxS",
        "outputId": "edf215ed-8664-4999-b32c-e5dc66f03d03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.12-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.2.2)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
            "Downloading pytools-2024.1.12-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp310-cp310-linux_x86_64.whl size=662246 sha256=ddc1a19e42908d77a5a5fe82bcdf872ff43b86325babd8e36a6d05319add6981\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/63/40/4bf006182f942d3516b71bb2ff3b57ccbdb8b2c0ee81882b6e\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.5 pycuda-2024.1.2 pytools-2024.1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "print (\"%d device(s) found.\" % drv.Device.count())\n",
        "for ordinal in range(drv.Device.count()):\n",
        "  dev = drv.Device(ordinal)\n",
        "  print (\"Device #%d: %s\" % (ordinal, dev.name()))\n",
        "  print (\" Compute Capability: %d.%d\" % dev.compute_capability())\n",
        "  print (\" Total Memory: %s KB\" % (dev.total_memory()//(1024)))\n",
        "  print (\" Maximum number of threads per block %s\" % str(dev.MAX_THREADS_PER_BLOCK))\n",
        "  print (dev.MULTIPROCESSOR_COUNT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAL2HBWMkXox",
        "outputId": "04218f80-6397-4318-b51b-3d30f0e7eabb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 device(s) found.\n",
            "Device #0: Tesla T4\n",
            " Compute Capability: 7.5\n",
            " Total Memory: 15464512 KB\n",
            " Maximum number of threads per block 1024\n",
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM4vHb7L3gqQ",
        "outputId": "599e6186-0670-4217-a52e-bd29a0d51d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-b4609bd1-0ecc-06b1-b454-6bd0bd6f4877)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy\n",
        "drive.mount('/content/drive')\n",
        "songs = pd.read_csv('/content/drive/My Drive/Colab Notebooks/yataros1.csv',header=None)\n",
        "\n",
        "dataset_sec_rec=songs.copy()\n",
        "dataset_sec_rec=dataset_sec_rec.transpose()\n",
        "\n",
        "dataset_sec_rec.drop(0, axis=1, inplace = True)\n",
        "dataset_sec_rec = dataset_sec_rec.iloc[1:]\n",
        "dataset_sec_rec = dataset_sec_rec.reset_index(drop=True)\n",
        "dataset_sec_rec = dataset_sec_rec.to_numpy()\n",
        "dataset_sec_rec=dataset_sec_rec.astype(numpy.float32)\n",
        "\n",
        "# songs=songs.transpose()\n",
        "# print(songs.isnull().sum())\n",
        "\n",
        "songs.drop(0, axis=1, inplace = True)\n",
        "songs = songs.iloc[1:]\n",
        "songs = songs.reset_index(drop=True)\n",
        "dataset = songs.to_numpy()\n",
        "\n",
        "# dataset=dataset[0:10,0:12]\n",
        "dataset=dataset.astype(numpy.float32)\n",
        "n=numpy.size(dataset,axis=0)\n",
        "# print(numpy.size(dataset,axis=0),numpy.size(dataset,axis=1))\n",
        "# print(dataset)\n",
        "cents_graph_global=[]\n",
        "# print(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqbCfMykkAfM",
        "outputId": "ab30bc72-1a54-4a31-8443-19cba1456609"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##PYCUDA\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy\n",
        "import math\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void edm(float *X_d,int N,int F,float *D_d)\n",
        "{\n",
        "\n",
        "int thisRow=threadIdx.y+blockIdx.y*blockDim.y;\n",
        "int otherRow=threadIdx.x+blockIdx.x*blockDim.x;\n",
        "float d;\n",
        "int m;\n",
        "\n",
        "if (thisRow< F && otherRow <F){\n",
        "  d=0;\n",
        "  for(int f = 0; f < N; f++) {\n",
        "    int i=thisRow*N+f;\n",
        "    int j=otherRow*N+f;\n",
        "    d=d+pow((X_d[i]-X_d[j]),2);\n",
        "  }\n",
        "  m=thisRow*F+otherRow;\n",
        "  D_d[m]=sqrt((float) d);\n",
        "}\n",
        "}\n",
        "\n",
        "__global__ void KCentresGPU(float *D_d,int *K_d,int *L_d,int N, int K)\n",
        "{\n",
        "int i=threadIdx.x+blockIdx.x*blockDim.x;\n",
        "float minimum=float('inf');\n",
        "int pos=-1;\n",
        "int kk,m;\n",
        "\n",
        "if (i<N){\n",
        "  for(int k = 0; k < K; k++) {\n",
        "    kk=K_d[k];\n",
        "    m=i*N+kk;\n",
        "\n",
        "    if (D_d[m]<minimum){\n",
        "      minimum=D_d[m];\n",
        "      pos=k;\n",
        "    }\n",
        "  }\n",
        "  L_d[i]=pos;\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void findMaximaGPU(float *D_d,int *L_d,float *M_d,int N)\n",
        "{\n",
        "int i=threadIdx.x+blockIdx.x*blockDim.x;\n",
        "float maximum=-1.0;\n",
        "\n",
        "int m1,m2;\n",
        "\n",
        "if (i<N){\n",
        "  int cluster_label=L_d[i];\n",
        "  for(int j = 0; j < N; j++) {\n",
        "    if(cluster_label==L_d[j]){\n",
        "      m1=i*N+j;\n",
        "      if(D_d[m1]>maximum){\n",
        "        maximum=D_d[m1];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  m2=cluster_label*N+i;\n",
        "  M_d[m2]=maximum;\n",
        "\n",
        "}\n",
        "}\n",
        "\n",
        "__global__ void findMinimumOfMaximaGPU(int *t_d,float *M_d,int N,int K)\n",
        "{\n",
        "  int k=threadIdx.x+blockIdx.x*blockDim.x;\n",
        "  float minimum=float('inf');\n",
        "  int m3,pos;\n",
        "\n",
        "  if(k<K){\n",
        "    for(int j=0;j<N;j++){\n",
        "      m3=k*N+j;\n",
        "      if(M_d[m3]<minimum){\n",
        "        minimum=M_d[m3];\n",
        "        pos=j;\n",
        "      }\n",
        "    }\n",
        "    t_d[k]=pos;\n",
        "  }\n",
        "}\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "7r3_UwhxnR_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fffc3f9-4b8e-4c5e-f804-85549a7d82aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-486ab84a048e>:9: UserWarning: The CUDA compiler succeeded, but said the following:\n",
            "kernel.cu(26): warning #1422-D: multicharacter character literal (potential portability problem)\n",
            "  float minimum=float('inf');\n",
            "                      ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "kernel.cu(71): warning #1422-D: multicharacter character literal (potential portability problem)\n",
            "    float minimum=float('inf');\n",
            "                        ^\n",
            "\n",
            "\n",
            "  mod = SourceModule(\"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9IyXTCvj0iZ",
        "outputId": "36d857b6-eeb1-4c30-f7c0-f0125c946a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9baa2b71d58a>:45: RuntimeWarning: overflow encountered in cast\n",
            "  mat_dis = mat_dis.astype(numpy.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de Matriz de distancias:  7.486343383789062e-05\n",
            "Tiempo de K-centres:  0.00038193702697753907\n",
            "\n",
            " [1.87371984e+08 9.36760559e+01 5.79447578e+04 1.35219893e+04\n",
            " 8.05443848e+02] [2 3 3 3 3 1 4 1 1 4 1 1 1 1 1 1 1 0 0 0 0 2 0 1 1 1 1 1 1 1 1 4 1 3 1 4 1\n",
            " 1 4 1]\n",
            "\n",
            "CENTROIDES : [19, 24, 0, 2, 9]\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer\n",
        "import time\n",
        "\n",
        "def findFirstCentres(k_length,n):\n",
        "  #L\n",
        "  L=numpy.full((n),-1,dtype=numpy.int32)\n",
        "  L_gpu=cuda.mem_alloc(L.nbytes)\n",
        "  cuda.memcpy_htod(L_gpu,L)\n",
        "\n",
        "  #t\n",
        "  t=numpy.full((k_length),-1,dtype=numpy.int32)\n",
        "  t_gpu=cuda.mem_alloc(t.nbytes)\n",
        "  cuda.memcpy_htod(t_gpu,t)\n",
        "\n",
        "  #M\n",
        "  M=numpy.full((k_length,n),numpy.inf)\n",
        "  M = M.astype(numpy.float32)\n",
        "  M_gpu = cuda.mem_alloc(M.nbytes)\n",
        "  cuda.memcpy_htod(M_gpu, M)\n",
        "\n",
        "\n",
        "  return L_gpu,L,t_gpu,t,M_gpu,M\n",
        "\n",
        "def main(dataset,k_length,n,nrep=1):\n",
        "  elapsed_time=0\n",
        "\n",
        "  L_gpu,L,t_gpu,t,M_gpu,M=findFirstCentres(k_length,n)\n",
        "\n",
        "  func_EDM = mod.get_function(\"edm\")\n",
        "  func_KCentresGPU = mod.get_function(\"KCentresGPU\")\n",
        "  func_findMaximaGPU = mod.get_function(\"findMaximaGPU\")\n",
        "  func_findMinimumOfMaximaGPU = mod.get_function(\"findMinimumOfMaximaGPU\")\n",
        "\n",
        "  dataset = dataset.astype(numpy.float32)\n",
        "  dataset_gpu = cuda.mem_alloc(dataset.nbytes)\n",
        "  cuda.memcpy_htod(dataset_gpu, dataset)\n",
        "\n",
        "  F=numpy.size(dataset, 1)\n",
        "  F= numpy.int32(F)\n",
        "  N= numpy.size(dataset,axis=0)\n",
        "  N= numpy.int32(N)\n",
        "  k_length= numpy.int32(k_length)\n",
        "\n",
        "  mat_dis=numpy.empty([F, F])\n",
        "  mat_dis = mat_dis.astype(numpy.float32)\n",
        "  mat_dis_gpu = cuda.mem_alloc(mat_dis.nbytes)\n",
        "  cuda.memcpy_htod(mat_dis_gpu, mat_dis)\n",
        "  N_mat_dis= numpy.size(mat_dis,axis=0)\n",
        "  N_mat_dis= numpy.int32(N_mat_dis)\n",
        "\n",
        "  threads=dev.MAX_THREADS_PER_BLOCK\n",
        "  threads1=math.sqrt(threads)\n",
        "  threads1= int(threads1)\n",
        "\n",
        "  gridx=int((N+threads1-1)/threads1)\n",
        "  gridy=int((N+threads1-1)/threads1)\n",
        "  inicio=default_timer()\n",
        "\n",
        "  ##MEDIDA\n",
        "  start_time=time.time()\n",
        "  func_EDM(dataset_gpu,N,F,mat_dis_gpu,block=(threads1,threads1,1),grid=(gridx,gridy,1))\n",
        "\n",
        "  end_time=time.time()\n",
        "  print(\"Tiempo de Matriz de distancias: \",end_time - start_time)\n",
        "\n",
        "  cuda.memcpy_dtoh(mat_dis, mat_dis_gpu)\n",
        "\n",
        "\n",
        "  dimx1 = int(math.sqrt(threads));\n",
        "  block1=(dimx1, 1,1);\n",
        "  grid1=(int((n + block1[0] - 1) / block1[0]), 1,1);\n",
        "  dopt=numpy.inf\n",
        "\n",
        "  cents_graph_pyCuda=[]\n",
        "  for i in range(nrep):\n",
        "\n",
        "\n",
        "    K=numpy.array([3, 14, 35, 24 , 39], dtype=numpy.int32)\n",
        "    K_gpu = cuda.mem_alloc(K.nbytes)\n",
        "    cuda.memcpy_htod(K_gpu, K)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "      ##MEDIDA TIEMPO\n",
        "      start_time=time.time()\n",
        "\n",
        "      func_KCentresGPU(mat_dis_gpu,K_gpu,L_gpu,N_mat_dis,k_length,block=block1,grid=grid1)\n",
        "\n",
        "      cuda.memcpy_dtoh(L, L_gpu)\n",
        "\n",
        "      func_findMaximaGPU(mat_dis_gpu,L_gpu,M_gpu,N_mat_dis,block=block1,grid=grid1)\n",
        "\n",
        "      func_findMinimumOfMaximaGPU(t_gpu,M_gpu,N_mat_dis,k_length,block=block1,grid=grid1)\n",
        "\n",
        "\n",
        "      end_time=time.time()\n",
        "      elapsed_time+= (end_time - start_time)\n",
        "\n",
        "\n",
        "      cuda.memcpy_dtoh(t, t_gpu)\n",
        "      # cuda.memcpy_dtoh(K, K_gpu)\n",
        "\n",
        "\n",
        "      if(numpy.all(t==K)):\n",
        "        dmin,labs=numpy.min(mat_dis[t,:],axis=0),numpy.argmin(mat_dis[t,:],axis=0)\n",
        "        dmin=numpy.max(dmin)\n",
        "        cents_graph_pyCuda.append(t.tolist())\n",
        "        break\n",
        "      K=t.copy()\n",
        "      # K_gpu=t_gpu\n",
        "      cuda.memcpy_dtod(K_gpu, t_gpu, K.nbytes)\n",
        "\n",
        "    finish_time = time.time()\n",
        "    elapsed_time+=(finish_time-start_time)\n",
        "\n",
        "    #almacena los centros más optimos en jopt.\n",
        "    if(dmin<=dopt):\n",
        "        dopt=dmin\n",
        "        labels=numpy.transpose(labs)\n",
        "        jopt=t.tolist()\n",
        "\n",
        "\n",
        "  #Determina los mejores centros sobre los N intentos.\n",
        "  dm=numpy.zeros(k_length)\n",
        "  for i in range(k_length):\n",
        "      L=numpy.where(labels==i)[0]\n",
        "      dm[i]=numpy.max(mat_dis[jopt[i],L])\n",
        "\n",
        "  cents_graph_global.append(cents_graph_pyCuda)\n",
        "\n",
        "  fin=default_timer()\n",
        "\n",
        "\n",
        "  print(\"Tiempo de K-centres: \",elapsed_time/nrep)\n",
        "\n",
        "  return labels,jopt,dm\n",
        "\n",
        "\n",
        "k_length=5\n",
        "\n",
        "n=numpy.size(dataset,axis=1)\n",
        "\n",
        "labels,jopt,dm=main(dataset,k_length,n,25)\n",
        "\n",
        "print(\"\\n\",dm,labels)\n",
        "print(\"\\nCENTROIDES :\",jopt)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}